  We now describe the algorithm for identifying the terms and
  expression of the equation from a sentence, and constructing a
  complete equation using them. We use two modules in a pipeline. They
  area s follows:
  \begin{enumerate}
    \item \textbf{Quantity Detector} : This module looks at a quantity
      and decides whether it is relevant in the equation formed from
      the sentence. Also, it is responsible for deciding whther the
      quantity acts as a modifier in a sentence, and how it 
      modifies the variables in the problem.
      
    \item \textbf{Equation Parse Generator} : This module jointly
      grounds variables and generates an equation parse.
  \end{enumerate}

  We discuss the details of these modules in the following subsections.
  
  \subsection{Quantities Extraction and Normalization}
    The natural first step towards identifying the terms participating
    in the equation is to find the quantities mentioned in the
    sentence. We use the Illinois Quantifier package \cite{Roy} to detect
    all the quantity mentions.  The quantifier extracts mention spans
    from the text, where each span identifies a quantity. Each such
    span is represented as a triple $(v,u,c)$ where $v$ denotes the value,
    $u$ denotes the unit and $c$ denotes the change in the quantity (if
    any).

    The quantity mentions identified fall into 3 categories:
    \begin{enumerate}
    \item {\bf Tree Mention} - These are the mentions that are part of
      the final equation tree. In Fig \ref{fig:eqparse}, ``5'' is a tree
      mention.

    \item {\bf Spurious Mention} - These are quantities which are not
      used in the equation.  In Fig \ref{fig:eqparse}, the quantity
      ``two numbers'' is a spurious mention, as the magnitude $2$
      plays no role in the final equation. This means that the
      quantity triggers arising from this mention will not be part of
      the equation tree.  Variable triggers generated using the same
      span can still be relevant to the equation tree.

    \item {\bf Modifier Mention} - These are quantities which play the
      role of modifiers. In Fig \ref{fig:eqparse}, ``10'' is a
      modifier mention, as it modifies both the variables in the
      resulting equation.
    \end{enumerate}

  \subsection{Quantity Classifier}
    As stated above, the quantifier extracts three types of quantity
    mentions : Tree Mention, Spurious Mention, and Modifier
    Mention. This module identifies the type to which a mention
    detected by the quantifier belongs. For the case of modifier
    mentions, this module also tries to detect how the modifier
    changes the variables in the problem. Since the number of outcomes
    is few, we predict these properties of quantities
    jointly. Specifically, we perform a multicalss classification to
    classify each quantity to be a Tree mention, Spurious mention, or
    a modifier conjoined with a type.

    The features used for this module are ...
    
  \subsection{Equation Tree Generator}

    This module receives the Tree mentions detected by the Quantity
    classifier, and predicts an equation tree along with grounding of
    variables. We formulate this problem as a structured prediction
    task.  The input structure $x$ consists of the problem sentence,
    as well as the tree mentions detected by the Quantity
    classifier. Grounding of variables might be ambiguous, since
    multiple groundings might be valid for any given
    variable. Therefore, we model the grounding of variables to be a
    latent variable $h$, and our system gets to decide which of the
    valid groundings it will predict. The output structure $y$ now comprises
    of an equation tree, whose leaves are formed either by the tree mentions
    in $x$, or the grounded variable mentions $h$.

    We want to have a scoring function to decide given $x$, the best
    equation tree $y^*$ and the best grounding of variables $h^*$. We
    assume our scoring function to be of the following form :
    \begin{align*}
      f_w(y, h) &= w^T\phi(x, y, h) \\ 
    \end{align*}  
    where $\phi(x, y, h)$ is a feature vector extracted from $x, h,
    y$. The prediction $(y^*, h^*)$ is then chosen as follows:
    \begin{align*}
      (y^*, h^*) &= \arg\max_{(y, h)\in H \times Y} f_w(y, h)
    \end{align*}

    \subsubsection{Learning}
      We assume access to $N$ training examples of the form : $(x_1,
      H_1, y_1), (x_2, H_2, y_2), \ldots, (x_N, H_N, y_N)$, where each
      $H_i$ is a set of valid groundings possible for the variables in
      the equation tree $y_i$. This is in contrast to most latent
      variable structured learning problems in that, the domain of the
      hidden variable is constrained by $H_i$. We use a modified
      latent structural SVM to learn the weight vector $w$. The algorithm 
      is given in 

      \begin{algorithm}
       \caption{Constrained Latent Structural SVM( $T$ )}
       \label{lssvm}
       \begin{algorithmic}[1]
         \REQUIRE Training data $T = (x_1, H_1, y_1), (x_2, H_2, y_2),
         \ldots, (x_N, H_N, y_N)$ \ENSURE Trained weight vector $w$
         \STATE w \leftarrow w_0
         \REPEAT 
           \STATE T' \leftarrow \emptyset
           \FORALL (x_i, y_i, H_i) \in T
             \STATE h_i^* \leftarrow \argmax_{h \in H_i} w^T\phi(x_i, y_i, h)
             \STATE T' \leftarrow T \cup \{x_i, (y_i, h_i^*)\}             
           \ENDFOR
           \STATE Update $w$ by running standard Structural SVM algorithm
           on T'
         \UNTIL{convergence}
         \RETURN w
       \end{algorithmic}
      \end{algorithm}


%      \begin{multline}
%        \min_w \left[ \frac{1}{2}||w||^2
%          + C\sum_{i=1}^N \max_{(\hat{y}, \hat{h})\in Y \times H}
%          \left[w\cdot \phi(x_i, \hat{y}, \hat{h}) 
%            + \Delta(y_i, \hat{y}, \hat{h}) \right] \right] \\
%        - \left[ C\sum_{i=1}^N \max_{h \in H_i} 
%          w \cdot \phi(x_i, y_i, h) \right]
%      \end{multline}
      
%      where $\Delta(y_i, \hat{y}, \hat{h})$ is a structured loss
%      suffered when, instead of gold structure $y_i$, the system
%      predicts equation tree $\hat{y}$, along with grounding
%      $\hat{h}$. 

      The distinction from standard latent structural SVM is in the latent
      variable completion problem, stated in Eq . In order to get the best 
      latent variable instantiation $h_i$ for input tuple $(x_i, y_i, H_i)$, 
      we search only inside $H_i$, instead of all possible values of $h_i$.

      

   \subsubsection{Inference}
     
      


    
    





