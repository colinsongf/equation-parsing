  We now describe the algorithm for identifying the terms and
  expression of the equation from a sentence, and constructing a
  complete equation using them. We use two modules in a pipeline. They
  area s follows:
  \begin{enumerate}
    \item \textbf{Quantity Detector} : This module looks at a quantity
      and decides whether it is relevant in the equation formed from
      the sentence. Also, it is responsible for deciding whther the
      quantity acts as a modifier in a sentence, and how it 
      modifies the variables in the problem.
      
    \item \textbf{Equation Parse Generator} : This module jointly
      grounds variables and generates an equation parse.
  \end{enumerate}

  We discuss the details of these modules in the following subsections.
  
  \subsection{Quantities Extraction and Normalization}
    The natural first step towards identifying the terms participating
    in the equation is to find the quantities mentioned in the
    sentence. We use the Illinois Quantifier package \cite{RoyViRo15}
    to detect all the quantity mentions.  The quantifier extracts
    mention spans from the text, where each span identifies a
    quantity. Each such span is represented as a triple $(v,u,c)$
    where $v$ denotes the value, $u$ denotes the unit and $c$ denotes
    the change in the quantity (if any).

    The quantity mentions identified fall into 3 categories:
    \begin{enumerate}
    \item {\bf Tree Mention} - These are the mentions that are part of
      the final equation tree. In Fig \ref{fig:eqparse}, ``5'' is a tree
      mention.

    \item {\bf Spurious Mention} - These are quantities which are not
      used in the equation.  In Fig \ref{fig:eqparse}, the quantity
      ``two numbers'' is a spurious mention, as the magnitude $2$
      plays no role in the final equation. This means that the
      quantity triggers arising from this mention will not be part of
      the equation tree.  Variable triggers generated using the same
      span can still be relevant to the equation tree.

    \item {\bf Modifier Mention} - These are quantities which play the
      role of modifiers. In Fig \ref{fig:eqparse}, ``10'' is a
      modifier mention, as it modifies both the variables in the
      resulting equation.
    \end{enumerate}

  \subsection{Quantity Classifier}
    As stated above, the quantifier extracts three types of quantity
    mentions : Tree Mention, Spurious Mention, and Modifier
    Mention. This module identifies the type to which a mention
    detected by the quantifier belongs. For the case of modifier
    mentions, this module also tries to detect how the modifier
    changes the variables in the problem. Since the number of outcomes
    is few, we predict these properties of quantities
    jointly. Specifically, we perform a multicalss classification to
    classify each quantity to be a Tree mention, Spurious mention, or
    a modifier conjoined with a type.

    The features used for this module are ...
    
  \subsection{Equation Tree Generator}

    This module receives the Tree mentions detected by the Quantity
    classifier, and predicts an equation tree along with grounding of
    variables. We formulate this problem as a structured prediction
    task.  The input structure $x$ consists of the problem sentence,
    as well as the tree mentions detected by the Quantity
    classifier. Grounding of variables might be ambiguous, since
    multiple groundings might be valid for any given
    variable. Therefore, we model the grounding of variables to be a
    latent variable $h$, and our system gets to decide which of the
    valid groundings it will predict. The output structure $y$ now comprises
    of an equation tree, whose leaves are formed either by the tree mentions
    in $x$, or the grounded variable mentions $h$.

    We want to have a scoring function to decide given $x$, the best
    equation tree $y^*$ and the best grounding of variables $h^*$. We
    assume our scoring function to be of the following form :
    \begin{align*}
      f_w(y, h) &= w^T\phi(x, y, h) \\ 
    \end{align*}  
    where $\phi(x, y, h)$ is a feature vector extracted from $x, h,
    y$. The prediction $(y^*, h^*)$ is then chosen as follows:
    \begin{align*}
      (y^*, h^*) &= \arg\max_{(y, h)\in H \times Y} f_w(y, h)
    \end{align*}

    \subsubsection{Learning}
      We assume access to $N$ training examples of the form : $(x_1,
      H_1, y_1), (x_2, H_2, y_2), \ldots, (x_N, H_N, y_N)$, where each
      $H_i$ is a set of valid groundings possible for the variables in
      the equation tree $y_i$. This is in contrast to most latent
      variable structured learning problems in that, the domain of the
      hidden variable is constrained by $H_i$. We use a modified
      latent structural SVM to learn the weight vector $w$. The algorithm 
      is given in 

      \begin{algorithm}
       \caption{Constrained Latent Structural SVM}
       \label{lssvm}
       \begin{algorithmic}[1]
         \REQUIRE Training data $T = \{(x_1, H_1, y_1), (x_2, H_2, y_2),
         \ldots, (x_N, H_N, y_N)\}$ 
         \ENSURE Trained weight vector $w$
         \STATE $w \leftarrow w_0$
         \REPEAT 
           \STATE $T' \leftarrow \emptyset$
           \FORALL {$(x_i, y_i, H_i) \in T$}
             \STATE $h_i^* \leftarrow \arg\max_{h \in H_i} w^T\phi(x_i, y_i, h)$
             \STATE $T' \leftarrow T \cup \{x_i, (y_i, h_i^*)\}$  
           \ENDFOR
           \STATE Update $w$ by running standard Structural SVM algorithm
           on $T'$
         \UNTIL{convergence}
         \RETURN $w$
       \end{algorithmic}
      \end{algorithm}


%      \begin{multline}
%        \min_w \left[ \frac{1}{2}||w||^2
%          + C\sum_{i=1}^N \max_{(\hat{y}, \hat{h})\in Y \times H}
%          \left[w\cdot \phi(x_i, \hat{y}, \hat{h}) 
%            + \Delta(y_i, \hat{y}, \hat{h}) \right] \right] \\
%        - \left[ C\sum_{i=1}^N \max_{h \in H_i} 
%          w \cdot \phi(x_i, y_i, h) \right]
%      \end{multline}
      
%      where $\Delta(y_i, \hat{y}, \hat{h})$ is a structured loss
%      suffered when, instead of gold structure $y_i$, the system
%      predicts equation tree $\hat{y}$, along with grounding
%      $\hat{h}$. 

      The distinction from standard latent structural SVM is in the
      latent variable completion problem, stated in Eq . In order to
      get the best latent variable assignment $h_i^*$ for input tuple
      $(x_i, y_i, H_i)$, we search only inside $H_i$, instead of all
      possible values of $h_i$. A similar formulation can be found in
      \cite{ZettlemoyerCo07,BjorkelundKu14}, but in both the cases,
      the set $H_i$ can be computed deterministically from $y_i$. In
      our case, we depend on explicit annotations for $H_i$.

    \subsubsection{Inference}
      We need to solve two inference algorithms to run the latent SSVM
      algorithm.  The first is the label completion problem, where we
      need to compute the best grounding $h^*$ given gold input $x_i$
      and gold equation tree $y_i$. We also have access to the set of
      valid groundings $H_i$, which reduces the search space
      significantly, and allows us to do exhaustive
      search. Specifically, we look at all nouns, verbs and
      adjectives, which are valid groundings of the variables used in
      $y_i$, and choose the one with the best score.

      The second inference algorithm is the one used in the standard
      structural SVM algorithm. Here we need to compute
      \begin{align*}
        (y^*, h^*) = \arg\max_{(y, h) \in Y \times H} w^T\phi(x, y, h)
      \end{align*}  
      Given an input sentence and the corresponding tree mentions, we
      need to find the best grounding of the variables $h^*$ as well
      as the best equation tree $y^*$. Here we also do not have access
      to candidate gold groundings, and exhaustive search will entail
      looking at exponential number of possibilities. Therefore, we
      use a beam search procedure to approximately find the best
      grounding and equation tree. First we enumerate all possible
      groundings for both single variable and two variable problems,
      and initialize our beam with top-k best groundings. Once the
      groundings are available, we know what the leaves of the
      equation tree are. Hence for each grounding in our beam, we now
      employ a CKY algorithm to construct the trees in a bottom up
      fashion. Since the independence assumptions needed for CKY are
      not valid here, we still use beam search, maintaining the best k
      parses at each cell of the CKY table. 
      
    \subsubsection{Knowledge Base} 
      In language expressing mathematical relations, often the
      information about the relation is hidden in a few key tokens or
      expressions. A list of such indicator tokens can help the system
      in extracting more meaningful features. Fortunately, for math
      word problems, such a list is readily available. Various
      coaching websites provide a list of tokens or phrases, which can
      help students distinguish which type of problem it is. We computed 
      a list of such indicator tokens or variables by combining the lists 
      from two websites.
      


      
      


    
    





